{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# Stat-Arb \u2014 Exploratory Walkthrough\n",
        "\n",
        "This notebook is a lightweight, end-to-end demo of the core pipeline:\n",
        "\n",
        "**prices \u2192 returns \u2192 covariance \u2192 factors \u2192 residuals \u2192 signals \u2192 backtest**\n",
        "\n",
        "Notes:\n",
        "- Prefer a small `data/sample_prices.csv` for reproducibility.\n",
        "- Large datasets should be pulled externally and are not committed.\n",
        "- This notebook assumes your reusable logic lives in `src/` (e.g., `src/factors.py`, `src/residuals.py`, `src/signals.py`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false,
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "source": [
        "## Contents\n",
        "- [0. Setup](#0-setup)\n",
        "- [1. Load Data](#1-load-data)\n",
        "- [2. Returns](#2-returns)\n",
        "- [3. Covariance](#3-covariance)\n",
        "- [4. Factors (Rolling PCA)](#4-factors-rolling-pca)\n",
        "- [5. Residuals + Signals](#5-residuals--signals)\n",
        "- [6. Backtest + Save Results](#6-backtest--save-results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "This cell sets up imports and ensures we can import from `src/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# Make sure we can import from ../src when running inside notebooks/\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "print(\"Project root:\", PROJECT_ROOT)\n",
        "print(\"Python path ok:\", str(PROJECT_ROOT) in sys.path)\n",
        "\n",
        "# Import your pipeline modules\n",
        "from src.factors import run_rolling_pca, compute_factor_returns\n",
        "from src.residuals import compute_eps_last, build_spread_from_eps\n",
        "from src.signals import rolling_zscore, alpha_from_z\n",
        "\n",
        "print(\"Imported src modules successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false
      },
      "source": [
        "## 1. Load Data\n",
        "\n",
        "Recommended: commit a small sample dataset at `data/sample_prices.csv`.\n",
        "\n",
        "If you need to pull the full dataset externally, place it in `data/` locally (not committed) and set `USE_SAMPLE = False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "# === Choose one ===\n",
        "USE_SAMPLE = True\n",
        "\n",
        "SAMPLE_PATH = PROJECT_ROOT / \"data\" / \"sample_prices.csv\"\n",
        "FULL_PATH   = PROJECT_ROOT / \"data\" / \"merged_data.csv\"  # example placeholder (not committed)\n",
        "\n",
        "data_path = SAMPLE_PATH if USE_SAMPLE else FULL_PATH\n",
        "print(\"Using:\", data_path)\n",
        "\n",
        "if not data_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not find {data_path}. \"\n",
        "        \"If using sample, add data/sample_prices.csv. \"\n",
        "        \"If using full data, download it to data/merged_data.csv (not committed).\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "# Basic loader for Bloomberg-style exports:\n",
        "# - often first 1\u20132 rows are metadata\n",
        "# - first column is date\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# If your file has two metadata rows, uncomment:\n",
        "# df = df.iloc[2:].copy()\n",
        "\n",
        "date_col = df.columns[0]\n",
        "df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "df = df.dropna(subset=[date_col]).set_index(date_col).sort_index()\n",
        "\n",
        "# numeric coercion\n",
        "for c in df.columns:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# forward fill small gaps (avoid bfill to prevent look-ahead)\n",
        "prices = df.dropna(how=\"all\").ffill()\n",
        "\n",
        "print(\"prices shape:\", prices.shape)\n",
        "display(prices.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false
      },
      "source": [
        "## 2. Returns\n",
        "\n",
        "Compute log returns and apply light winsorization for robustness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "WINSOR_Q = 0.005\n",
        "\n",
        "rets = np.log(prices).diff()\n",
        "\n",
        "# winsorize per column\n",
        "ql = rets.quantile(WINSOR_Q)\n",
        "qh = rets.quantile(1 - WINSOR_Q)\n",
        "rets = rets.clip(lower=ql, upper=qh, axis=1)\n",
        "rets = rets.dropna(how=\"all\")\n",
        "\n",
        "print(\"rets shape:\", rets.shape)\n",
        "display(rets.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false
      },
      "source": [
        "## 3. Covariance\n",
        "\n",
        "Estimate covariance on a recent window (default: last 252 days). Optionally use Ledoit\u2013Wolf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "from sklearn.covariance import LedoitWolf\n",
        "\n",
        "WINDOW_DAYS = 252\n",
        "R = rets.tail(WINDOW_DAYS)\n",
        "\n",
        "# drop columns with too much missing data in the window\n",
        "min_coverage = 0.8\n",
        "keep = (R.notna().mean(axis=0) >= min_coverage)\n",
        "R = R.loc[:, keep]\n",
        "\n",
        "# complete-case for LW fit\n",
        "R_cc = R.dropna(axis=0, how=\"any\")\n",
        "print(\"Window complete-case shape:\", R_cc.shape)\n",
        "\n",
        "Sigma_sample = R.cov()\n",
        "\n",
        "lw = LedoitWolf().fit(R_cc.values)\n",
        "Sigma_lw = pd.DataFrame(lw.covariance_, index=R_cc.columns, columns=R_cc.columns)\n",
        "\n",
        "print(\"Sample cov shape:\", Sigma_sample.shape)\n",
        "print(\"Ledoit\u2013Wolf cov shape:\", Sigma_lw.shape)\n",
        "print(\"LW shrinkage:\", float(lw.shrinkage_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false
      },
      "source": [
        "## 4. Factors (Rolling PCA)\n",
        "\n",
        "Run rolling PCA (your implementation in `src/factors.py`) and produce factor returns.\n",
        "\n",
        "Tip: keep this fast by using a smaller universe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "UNIVERSE_N = min(150, rets.shape[1])\n",
        "rets_u = rets.iloc[:, :UNIVERSE_N].copy()\n",
        "print(\"Universe returns shape:\", rets_u.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "# Rolling PCA over trailing window\n",
        "L = 252\n",
        "\n",
        "rolling = run_rolling_pca(\n",
        "    rets_u,\n",
        "    L=L,\n",
        "    min_coverage=0.80,\n",
        "    var_threshold=0.55,\n",
        ")\n",
        "\n",
        "print(\"Rolling PCA points:\", len(rolling))\n",
        "if len(rolling) == 0:\n",
        "    raise ValueError(\"Rolling PCA returned no results. Try lowering min_coverage or reducing NaNs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "# Factor returns f_t = U^T z_t (per your src/factors.py)\n",
        "fac = compute_factor_returns(\n",
        "    rets_u,\n",
        "    rolling,\n",
        "    L=L,\n",
        "    max_pcs=10,\n",
        "    min_rows=180,\n",
        ")\n",
        "\n",
        "print(\"factor_returns shape:\", fac.shape)\n",
        "display(fac.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "# Diagnostic: explained variance over time (from RollingPCAResult)\n",
        "explained = pd.Series(\n",
        "    data=[r.explained for r in rolling],\n",
        "    index=pd.to_datetime([r.date for r in rolling]),\n",
        "    name=\"explained\",\n",
        ").sort_index()\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.plot(explained.index, explained.values)\n",
        "plt.title(\"Rolling PCA: Explained Variance (chosen k)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Explained variance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false
      },
      "source": [
        "## 5. Residuals + Signals\n",
        "\n",
        "Compute per-name residuals from factor regressions, build a cumulative residual series per name, and convert to a z-score signal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "# Residual at the LAST day of each lookback regression window\n",
        "eps_last = compute_eps_last(\n",
        "    rets=rets_u,\n",
        "    factor_returns=fac,\n",
        "    lookback=60,\n",
        "    min_rows=45,\n",
        ")\n",
        "\n",
        "print(\"eps_last shape:\", eps_last.shape)\n",
        "display(eps_last.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "# Spread proxy per name: cumulative residuals (NaNs treated as 0 to keep continuity)\n",
        "X = build_spread_from_eps(eps_last)\n",
        "print(\"X shape:\", X.shape)\n",
        "\n",
        "# Rolling z-score of spreads\n",
        "z = rolling_zscore(X, lookback=252, min_periods=126)\n",
        "\n",
        "# Mean reversion alpha: alpha = -z (clipped)\n",
        "alpha = alpha_from_z(z, clip=3.0)\n",
        "\n",
        "display(alpha.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false
      },
      "source": [
        "## 6. Backtest + Save Results\n",
        "\n",
        "A minimal, honest backtest:\n",
        "- Convert alpha into daily cross-sectional weights\n",
        "- Dollar-neutral (demean each day)\n",
        "- Normalize to target gross exposure\n",
        "- **Lag weights by 1 day** to avoid look-ahead\n",
        "- Save `results/backtest.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "hide_input": false
      },
      "outputs": [],
      "source": [
        "def make_weights_from_alpha(alpha: pd.DataFrame, gross_target: float = 1.0) -> pd.DataFrame:\n",
        "    a = alpha.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "    # Dollar-neutral each day\n",
        "    a = a.sub(a.mean(axis=1), axis=0)\n",
        "\n",
        "    # Scale to target gross exposure\n",
        "    gross = a.abs().sum(axis=1).replace(0.0, np.nan)\n",
        "    w = a.div(gross, axis=0) * gross_target\n",
        "    return w.fillna(0.0)\n",
        "\n",
        "weights = make_weights_from_alpha(alpha, gross_target=1.0)\n",
        "\n",
        "# Align to returns index, lag by 1 to avoid look-ahead\n",
        "weights = weights.reindex(rets_u.index).fillna(0.0)\n",
        "w_lag = weights.shift(1).fillna(0.0)\n",
        "\n",
        "pnl = (w_lag * rets_u).sum(axis=1)\n",
        "turnover = weights.diff().abs().sum(axis=1)\n",
        "gross = weights.abs().sum(axis=1)\n",
        "\n",
        "bt = pd.DataFrame(\n",
        "    {\n",
        "        \"pnl\": pnl,\n",
        "        \"cum_pnl\": pnl.cumsum(),\n",
        "        \"gross\": gross,\n",
        "        \"turnover\": turnover,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Basic performance stats\n",
        "pnl_std = bt[\"pnl\"].std(ddof=1)\n",
        "sharpe = np.nan\n",
        "if pnl_std and np.isfinite(pnl_std) and pnl_std > 0:\n",
        "    sharpe = float(bt[\"pnl\"].mean() / pnl_std * np.sqrt(252))\n",
        "\n",
        "print(\"Sharpe (daily, annualized):\", sharpe)\n",
        "display(bt.tail())\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.plot(bt.index, bt[\"cum_pnl\"].values)\n",
        "plt.title(\"Cumulative PnL (toy backtest)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Cumulative PnL\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "out_path = RESULTS_DIR / \"backtest.csv\"\n",
        "bt.to_csv(out_path, index=True)\n",
        "print(\"Saved:\", out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false
      },
      "source": [
        "## Next steps (optional)\n",
        "\n",
        "If you want to improve realism next:\n",
        "- Transaction costs (use turnover)\n",
        "- Position limits and liquidity filters\n",
        "- Signal smoothing / holding periods\n",
        "- A real pair or cluster construction step (market segmentation)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
